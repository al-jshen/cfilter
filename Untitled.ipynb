{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(path):\n",
    "    with open(path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        labs = next(reader)\n",
    "        dat = np.array(list(reader))\n",
    "    return(labs, dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_labels, ratings = open_file('./ratings.csv')\n",
    "movie_labels, movies = open_file('./movies.csv')\n",
    "\n",
    "# ratings[:,[0,1]] = ratings[:,[0,1]].astype(int) #doesnt work??? probably something to do with handling of mixed data types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['userId', 'movieId', 'rating', 'timestamp'] \n",
      "\n",
      "[['1' '1' '4.0' '964982703']\n",
      " ['1' '3' '4.0' '964981247']\n",
      " ['1' '6' '4.0' '964982224']\n",
      " ['1' '47' '5.0' '964983815']\n",
      " ['1' '50' '5.0' '964982931']]\n"
     ]
    }
   ],
   "source": [
    "print(rating_labels, '\\n')\n",
    "print(ratings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movieId', 'title', 'genres'] \n",
      "\n",
      "[['1' 'Toy Story (1995)' 'Adventure|Animation|Children|Comedy|Fantasy']\n",
      " ['2' 'Jumanji (1995)' 'Adventure|Children|Fantasy']\n",
      " ['3' 'Grumpier Old Men (1995)' 'Comedy|Romance']\n",
      " ['4' 'Waiting to Exhale (1995)' 'Comedy|Drama|Romance']\n",
      " ['5' 'Father of the Bride Part II (1995)' 'Comedy']]\n"
     ]
    }
   ],
   "source": [
    "print(movie_labels, '\\n')\n",
    "print(movies[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610\n",
      "9742\n"
     ]
    }
   ],
   "source": [
    "num_users = len(set(ratings[:,0])); print(num_users)\n",
    "num_movies = len(set(movies[:,0])); print(num_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ids = sorted(set(ratings[:,1]), key=lambda x: int(x))\n",
    "old_new_movie_ids = {j:i for i,j in enumerate(movie_ids)}\n",
    "movies[:,0] = np.array(list(map(old_new_movie_ids.get, movies[:,0])))\n",
    "ratings[:,1] = np.array(list(map(old_new_movie_ids.get, ratings[:,1])))\n",
    "ratings[:,0] = ratings[:,0].astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_matrix = np.empty((num_users, num_movies))\n",
    "rate_matrix[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in ratings:\n",
    "    rate_matrix[int(row[0]), int(row[1])] = row[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0000,    nan, 4.0000,  ...,    nan,    nan,    nan],\n",
      "        [   nan,    nan,    nan,  ...,    nan,    nan,    nan],\n",
      "        [   nan,    nan,    nan,  ...,    nan,    nan,    nan],\n",
      "        ...,\n",
      "        [2.5000, 2.0000, 2.0000,  ...,    nan,    nan,    nan],\n",
      "        [3.0000,    nan,    nan,  ...,    nan,    nan,    nan],\n",
      "        [5.0000,    nan,    nan,  ...,    nan,    nan,    nan]])\n"
     ]
    }
   ],
   "source": [
    "rate_matrix = torch.tensor(rate_matrix).float()\n",
    "print(rate_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testnet:\n",
    "    def __init__(self, num_users, num_movies, num_features):\n",
    "        self.user_embed = nn.init.xavier_uniform_(torch.zeros(num_users, num_features, requires_grad=True))\n",
    "        self.movie_embed = nn.init.xavier_uniform_(torch.zeros(num_features, num_movies, requires_grad=True))\n",
    "        \n",
    "        self.user_bias = nn.init.xavier_uniform_(torch.zeros(num_users, 1, requires_grad=True))\n",
    "        self.movie_bias = nn.init.xavier_uniform_(torch.zeros(1, num_movies, requires_grad=True))\n",
    "        \n",
    "    def forward(self):\n",
    "        out = torch.mm(self.user_embed, self.movie_embed) + self.user_bias + self.movie_bias\n",
    "        out = torch.sigmoid(out)\n",
    "        return(out * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.testnet'>\n"
     ]
    }
   ],
   "source": [
    "t = testnet(num_users, num_movies, 50)\n",
    "print(testnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_loss(pred, label):\n",
    "    diff = pred - label\n",
    "    sq = diff ** 2\n",
    "    mask = ~ torch.isnan(sq)\n",
    "    n_items = mask.sum()\n",
    "    loss = torch.masked_select(sq, mask).sum()\n",
    "    return(loss / n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = c_loss\n",
    "optimizer = optim.SparseAdam([x for x in t.__dict__.values()], lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "SparseAdam does not support dense gradients, please consider Adam instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-c6fcaf709534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/optim/sparse_adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SparseAdam does not support dense gradients, please consider Adam instead'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: SparseAdam does not support dense gradients, please consider Adam instead"
     ]
    }
   ],
   "source": [
    "for e in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = t.forward()\n",
    "    loss = loss_fn(output, rate_matrix)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (e+1) % 10 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = c_loss(t.forward(), rate_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_embed': tensor([[-0.0525, -0.0213,  0.0281,  ...,  0.0745,  0.0821, -0.0697],\n",
       "         [ 0.0364, -0.0202, -0.0738,  ...,  0.0807,  0.0037,  0.0535],\n",
       "         [-0.0724,  0.0746,  0.0950,  ...,  0.0625,  0.0154, -0.0937],\n",
       "         ...,\n",
       "         [-0.0167, -0.0421,  0.0843,  ...,  0.0505,  0.0604, -0.0341],\n",
       "         [-0.0606,  0.0342, -0.0757,  ...,  0.0066,  0.0632, -0.0598],\n",
       "         [-0.0697, -0.0418, -0.0395,  ...,  0.0637,  0.0464,  0.0361]],\n",
       "        requires_grad=True),\n",
       " 'movie_embed': tensor([[ 0.0225,  0.0175,  0.0226,  ..., -0.0237,  0.0049, -0.0219],\n",
       "         [-0.0165,  0.0203,  0.0128,  ...,  0.0158, -0.0180, -0.0186],\n",
       "         [-0.0168, -0.0163,  0.0157,  ..., -0.0185, -0.0104,  0.0068],\n",
       "         ...,\n",
       "         [ 0.0006,  0.0064,  0.0172,  ..., -0.0197, -0.0032,  0.0203],\n",
       "         [-0.0142,  0.0111,  0.0039,  ...,  0.0034, -0.0094, -0.0040],\n",
       "         [-0.0044,  0.0161,  0.0091,  ..., -0.0177,  0.0070,  0.0016]],\n",
       "        requires_grad=True),\n",
       " 'user_bias': tensor([[ 0.0037],\n",
       "         [-0.0779],\n",
       "         [ 0.0421],\n",
       "         [-0.0041],\n",
       "         [ 0.0377],\n",
       "         [-0.0366],\n",
       "         [-0.0687],\n",
       "         [ 0.0980],\n",
       "         [-0.0512],\n",
       "         [-0.0810],\n",
       "         [ 0.0106],\n",
       "         [-0.0914],\n",
       "         [ 0.0783],\n",
       "         [-0.0631],\n",
       "         [-0.0438],\n",
       "         [-0.0991],\n",
       "         [-0.0834],\n",
       "         [-0.0481],\n",
       "         [ 0.0288],\n",
       "         [ 0.0721],\n",
       "         [ 0.0357],\n",
       "         [-0.0679],\n",
       "         [-0.0205],\n",
       "         [ 0.0589],\n",
       "         [-0.0940],\n",
       "         [ 0.0567],\n",
       "         [ 0.0454],\n",
       "         [ 0.0105],\n",
       "         [-0.0647],\n",
       "         [ 0.0704],\n",
       "         [ 0.0778],\n",
       "         [ 0.0629],\n",
       "         [-0.0072],\n",
       "         [ 0.0610],\n",
       "         [-0.0898],\n",
       "         [-0.0133],\n",
       "         [ 0.0051],\n",
       "         [-0.0967],\n",
       "         [ 0.0891],\n",
       "         [-0.0110],\n",
       "         [ 0.0174],\n",
       "         [ 0.0422],\n",
       "         [ 0.0225],\n",
       "         [ 0.0967],\n",
       "         [ 0.0357],\n",
       "         [ 0.0839],\n",
       "         [-0.0442],\n",
       "         [ 0.0271],\n",
       "         [ 0.0668],\n",
       "         [-0.0223],\n",
       "         [ 0.0156],\n",
       "         [ 0.0309],\n",
       "         [ 0.0579],\n",
       "         [-0.0651],\n",
       "         [-0.0070],\n",
       "         [ 0.0173],\n",
       "         [ 0.0735],\n",
       "         [ 0.0456],\n",
       "         [-0.0247],\n",
       "         [-0.0457],\n",
       "         [ 0.0860],\n",
       "         [-0.0683],\n",
       "         [ 0.0634],\n",
       "         [ 0.0616],\n",
       "         [-0.0784],\n",
       "         [-0.0199],\n",
       "         [ 0.0127],\n",
       "         [ 0.0535],\n",
       "         [ 0.0193],\n",
       "         [ 0.0623],\n",
       "         [-0.0314],\n",
       "         [ 0.0442],\n",
       "         [-0.0355],\n",
       "         [-0.0675],\n",
       "         [ 0.0081],\n",
       "         [-0.0452],\n",
       "         [-0.0970],\n",
       "         [ 0.0504],\n",
       "         [-0.0969],\n",
       "         [ 0.0599],\n",
       "         [ 0.0710],\n",
       "         [ 0.0251],\n",
       "         [ 0.0147],\n",
       "         [ 0.0812],\n",
       "         [-0.0389],\n",
       "         [ 0.0738],\n",
       "         [-0.0753],\n",
       "         [ 0.0155],\n",
       "         [-0.0832],\n",
       "         [ 0.0488],\n",
       "         [-0.0642],\n",
       "         [-0.0547],\n",
       "         [-0.0344],\n",
       "         [ 0.0313],\n",
       "         [-0.0981],\n",
       "         [ 0.0826],\n",
       "         [ 0.0259],\n",
       "         [-0.0324],\n",
       "         [ 0.0551],\n",
       "         [-0.0375],\n",
       "         [ 0.0960],\n",
       "         [-0.0650],\n",
       "         [ 0.0629],\n",
       "         [ 0.0516],\n",
       "         [-0.0756],\n",
       "         [ 0.0923],\n",
       "         [ 0.0044],\n",
       "         [-0.0977],\n",
       "         [-0.0127],\n",
       "         [ 0.0857],\n",
       "         [-0.0486],\n",
       "         [ 0.0138],\n",
       "         [-0.0017],\n",
       "         [ 0.0304],\n",
       "         [ 0.0453],\n",
       "         [-0.0440],\n",
       "         [ 0.0582],\n",
       "         [-0.0089],\n",
       "         [ 0.0112],\n",
       "         [ 0.0031],\n",
       "         [ 0.0865],\n",
       "         [-0.0760],\n",
       "         [-0.0128],\n",
       "         [ 0.0195],\n",
       "         [ 0.0025],\n",
       "         [-0.0298],\n",
       "         [-0.0247],\n",
       "         [ 0.0323],\n",
       "         [-0.0216],\n",
       "         [-0.0950],\n",
       "         [ 0.0743],\n",
       "         [-0.0143],\n",
       "         [-0.0792],\n",
       "         [ 0.0155],\n",
       "         [-0.0794],\n",
       "         [ 0.0821],\n",
       "         [ 0.0947],\n",
       "         [ 0.0276],\n",
       "         [ 0.0006],\n",
       "         [-0.0537],\n",
       "         [-0.0785],\n",
       "         [ 0.0959],\n",
       "         [-0.0030],\n",
       "         [-0.0937],\n",
       "         [-0.0002],\n",
       "         [-0.0215],\n",
       "         [-0.0312],\n",
       "         [ 0.0410],\n",
       "         [ 0.0952],\n",
       "         [-0.0406],\n",
       "         [ 0.0576],\n",
       "         [-0.0341],\n",
       "         [ 0.0281],\n",
       "         [-0.0658],\n",
       "         [-0.0367],\n",
       "         [ 0.0410],\n",
       "         [-0.0691],\n",
       "         [-0.0056],\n",
       "         [-0.0903],\n",
       "         [-0.0233],\n",
       "         [-0.0287],\n",
       "         [-0.0662],\n",
       "         [-0.0231],\n",
       "         [-0.0928],\n",
       "         [ 0.0629],\n",
       "         [ 0.0820],\n",
       "         [ 0.0711],\n",
       "         [-0.0919],\n",
       "         [-0.0743],\n",
       "         [-0.0586],\n",
       "         [-0.0822],\n",
       "         [ 0.0921],\n",
       "         [ 0.0244],\n",
       "         [ 0.0223],\n",
       "         [ 0.0850],\n",
       "         [-0.0042],\n",
       "         [-0.0304],\n",
       "         [-0.0747],\n",
       "         [-0.0355],\n",
       "         [-0.0065],\n",
       "         [ 0.0218],\n",
       "         [-0.0443],\n",
       "         [-0.0823],\n",
       "         [ 0.0039],\n",
       "         [-0.0472],\n",
       "         [-0.0421],\n",
       "         [-0.0402],\n",
       "         [ 0.0536],\n",
       "         [-0.0980],\n",
       "         [ 0.0262],\n",
       "         [ 0.0149],\n",
       "         [-0.0542],\n",
       "         [-0.0526],\n",
       "         [-0.0053],\n",
       "         [ 0.0344],\n",
       "         [ 0.0411],\n",
       "         [ 0.0759],\n",
       "         [ 0.0494],\n",
       "         [-0.0654],\n",
       "         [-0.0496],\n",
       "         [ 0.0512],\n",
       "         [-0.0728],\n",
       "         [-0.0346],\n",
       "         [ 0.0287],\n",
       "         [ 0.0513],\n",
       "         [ 0.0537],\n",
       "         [-0.0662],\n",
       "         [ 0.0046],\n",
       "         [ 0.0450],\n",
       "         [-0.0362],\n",
       "         [ 0.0392],\n",
       "         [-0.0021],\n",
       "         [-0.0162],\n",
       "         [ 0.0351],\n",
       "         [ 0.0736],\n",
       "         [-0.0356],\n",
       "         [-0.0201],\n",
       "         [-0.0813],\n",
       "         [ 0.0403],\n",
       "         [-0.0342],\n",
       "         [-0.0744],\n",
       "         [ 0.0300],\n",
       "         [-0.0183],\n",
       "         [ 0.0420],\n",
       "         [-0.0521],\n",
       "         [-0.0916],\n",
       "         [-0.0057],\n",
       "         [ 0.0456],\n",
       "         [-0.0010],\n",
       "         [ 0.0569],\n",
       "         [-0.0472],\n",
       "         [-0.0589],\n",
       "         [ 0.0753],\n",
       "         [-0.0900],\n",
       "         [ 0.0022],\n",
       "         [ 0.0947],\n",
       "         [-0.0364],\n",
       "         [ 0.0444],\n",
       "         [-0.0157],\n",
       "         [-0.0347],\n",
       "         [-0.0033],\n",
       "         [ 0.0946],\n",
       "         [-0.0773],\n",
       "         [-0.0879],\n",
       "         [ 0.0833],\n",
       "         [ 0.0420],\n",
       "         [-0.0792],\n",
       "         [-0.0511],\n",
       "         [ 0.0394],\n",
       "         [ 0.0393],\n",
       "         [ 0.0830],\n",
       "         [ 0.0267],\n",
       "         [-0.0860],\n",
       "         [-0.0315],\n",
       "         [-0.0519],\n",
       "         [-0.0650],\n",
       "         [ 0.0097],\n",
       "         [-0.0638],\n",
       "         [ 0.0461],\n",
       "         [ 0.0073],\n",
       "         [-0.0016],\n",
       "         [-0.0426],\n",
       "         [-0.0957],\n",
       "         [ 0.0901],\n",
       "         [-0.0036],\n",
       "         [-0.0343],\n",
       "         [ 0.0212],\n",
       "         [-0.0208],\n",
       "         [-0.0529],\n",
       "         [ 0.0854],\n",
       "         [-0.0431],\n",
       "         [ 0.0866],\n",
       "         [ 0.0970],\n",
       "         [-0.0324],\n",
       "         [-0.0580],\n",
       "         [-0.0749],\n",
       "         [ 0.0269],\n",
       "         [ 0.0159],\n",
       "         [-0.0811],\n",
       "         [-0.0126],\n",
       "         [ 0.0189],\n",
       "         [-0.0067],\n",
       "         [ 0.0035],\n",
       "         [ 0.0158],\n",
       "         [-0.0090],\n",
       "         [ 0.0163],\n",
       "         [ 0.0823],\n",
       "         [ 0.0593],\n",
       "         [ 0.0436],\n",
       "         [-0.0031],\n",
       "         [-0.0174],\n",
       "         [-0.0479],\n",
       "         [ 0.0055],\n",
       "         [ 0.0696],\n",
       "         [-0.0051],\n",
       "         [ 0.0564],\n",
       "         [-0.0558],\n",
       "         [ 0.0901],\n",
       "         [-0.0146],\n",
       "         [ 0.0979],\n",
       "         [-0.0764],\n",
       "         [-0.0512],\n",
       "         [-0.0798],\n",
       "         [-0.0852],\n",
       "         [-0.0322],\n",
       "         [ 0.0233],\n",
       "         [ 0.0033],\n",
       "         [-0.0565],\n",
       "         [ 0.0400],\n",
       "         [-0.0615],\n",
       "         [-0.0253],\n",
       "         [-0.0832],\n",
       "         [-0.0558],\n",
       "         [ 0.0701],\n",
       "         [-0.0059],\n",
       "         [ 0.0103],\n",
       "         [ 0.0277],\n",
       "         [-0.0180],\n",
       "         [ 0.0003],\n",
       "         [ 0.0191],\n",
       "         [ 0.0839],\n",
       "         [ 0.0065],\n",
       "         [-0.0820],\n",
       "         [-0.0663],\n",
       "         [ 0.0710],\n",
       "         [ 0.0574],\n",
       "         [ 0.0932],\n",
       "         [-0.0708],\n",
       "         [ 0.0980],\n",
       "         [ 0.0245],\n",
       "         [-0.0878],\n",
       "         [ 0.0254],\n",
       "         [-0.0435],\n",
       "         [ 0.0891],\n",
       "         [ 0.0665],\n",
       "         [-0.0486],\n",
       "         [ 0.0757],\n",
       "         [-0.0029],\n",
       "         [ 0.0798],\n",
       "         [ 0.0548],\n",
       "         [-0.0500],\n",
       "         [ 0.0047],\n",
       "         [-0.0882],\n",
       "         [ 0.0085],\n",
       "         [ 0.0885],\n",
       "         [ 0.0898],\n",
       "         [ 0.0838],\n",
       "         [ 0.0924],\n",
       "         [ 0.0099],\n",
       "         [-0.0747],\n",
       "         [-0.0793],\n",
       "         [ 0.0480],\n",
       "         [ 0.0622],\n",
       "         [ 0.0725],\n",
       "         [-0.0457],\n",
       "         [-0.0973],\n",
       "         [-0.0645],\n",
       "         [ 0.0270],\n",
       "         [ 0.0484],\n",
       "         [-0.0572],\n",
       "         [ 0.0372],\n",
       "         [ 0.0705],\n",
       "         [ 0.0662],\n",
       "         [ 0.0947],\n",
       "         [ 0.0766],\n",
       "         [ 0.0625],\n",
       "         [ 0.0598],\n",
       "         [-0.0676],\n",
       "         [-0.0179],\n",
       "         [-0.0036],\n",
       "         [-0.0667],\n",
       "         [-0.0293],\n",
       "         [-0.0377],\n",
       "         [-0.0310],\n",
       "         [ 0.0310],\n",
       "         [-0.0808],\n",
       "         [ 0.0174],\n",
       "         [-0.0143],\n",
       "         [ 0.0659],\n",
       "         [ 0.0889],\n",
       "         [ 0.0281],\n",
       "         [ 0.0037],\n",
       "         [-0.0891],\n",
       "         [-0.0312],\n",
       "         [-0.0226],\n",
       "         [ 0.0346],\n",
       "         [-0.0416],\n",
       "         [ 0.0431],\n",
       "         [ 0.0669],\n",
       "         [-0.0940],\n",
       "         [-0.0495],\n",
       "         [-0.0497],\n",
       "         [-0.0443],\n",
       "         [-0.0715],\n",
       "         [-0.0593],\n",
       "         [-0.0666],\n",
       "         [-0.0597],\n",
       "         [ 0.0210],\n",
       "         [ 0.0678],\n",
       "         [-0.0964],\n",
       "         [ 0.0604],\n",
       "         [-0.0480],\n",
       "         [-0.0658],\n",
       "         [ 0.0954],\n",
       "         [-0.0491],\n",
       "         [ 0.0722],\n",
       "         [ 0.0498],\n",
       "         [-0.0566],\n",
       "         [-0.0770],\n",
       "         [ 0.0867],\n",
       "         [-0.0301],\n",
       "         [ 0.0497],\n",
       "         [-0.0385],\n",
       "         [ 0.0776],\n",
       "         [ 0.0041],\n",
       "         [ 0.0705],\n",
       "         [ 0.0212],\n",
       "         [-0.0429],\n",
       "         [-0.0648],\n",
       "         [ 0.0988],\n",
       "         [ 0.0389],\n",
       "         [ 0.0908],\n",
       "         [-0.0460],\n",
       "         [-0.0720],\n",
       "         [-0.0448],\n",
       "         [ 0.0106],\n",
       "         [-0.0848],\n",
       "         [ 0.0385],\n",
       "         [-0.0801],\n",
       "         [-0.0504],\n",
       "         [ 0.0403],\n",
       "         [ 0.0606],\n",
       "         [ 0.0071],\n",
       "         [-0.0279],\n",
       "         [ 0.0348],\n",
       "         [-0.0031],\n",
       "         [ 0.0271],\n",
       "         [ 0.0293],\n",
       "         [ 0.0139],\n",
       "         [ 0.0134],\n",
       "         [ 0.0174],\n",
       "         [ 0.0851],\n",
       "         [-0.0100],\n",
       "         [ 0.0026],\n",
       "         [ 0.0869],\n",
       "         [ 0.0930],\n",
       "         [ 0.0415],\n",
       "         [ 0.0666],\n",
       "         [-0.0160],\n",
       "         [-0.0525],\n",
       "         [-0.0643],\n",
       "         [-0.0163],\n",
       "         [ 0.0275],\n",
       "         [-0.0761],\n",
       "         [-0.0573],\n",
       "         [-0.0390],\n",
       "         [-0.0138],\n",
       "         [ 0.0841],\n",
       "         [ 0.0803],\n",
       "         [ 0.0688],\n",
       "         [-0.0049],\n",
       "         [ 0.0078],\n",
       "         [ 0.0877],\n",
       "         [ 0.0059],\n",
       "         [-0.0773],\n",
       "         [ 0.0514],\n",
       "         [ 0.0658],\n",
       "         [ 0.0591],\n",
       "         [-0.0806],\n",
       "         [ 0.0017],\n",
       "         [ 0.0775],\n",
       "         [ 0.0655],\n",
       "         [-0.0455],\n",
       "         [ 0.0010],\n",
       "         [ 0.0918],\n",
       "         [ 0.0518],\n",
       "         [-0.0863],\n",
       "         [-0.0971],\n",
       "         [ 0.0066],\n",
       "         [ 0.0134],\n",
       "         [ 0.0729],\n",
       "         [-0.0049],\n",
       "         [-0.0577],\n",
       "         [ 0.0086],\n",
       "         [ 0.0984],\n",
       "         [-0.0053],\n",
       "         [-0.0269],\n",
       "         [ 0.0233],\n",
       "         [ 0.0965],\n",
       "         [-0.0629],\n",
       "         [-0.0611],\n",
       "         [ 0.0667],\n",
       "         [ 0.0361],\n",
       "         [-0.0054],\n",
       "         [-0.0115],\n",
       "         [ 0.0854],\n",
       "         [-0.0725],\n",
       "         [-0.0187],\n",
       "         [ 0.0052],\n",
       "         [ 0.0518],\n",
       "         [ 0.0391],\n",
       "         [-0.0732],\n",
       "         [-0.0173],\n",
       "         [-0.0107],\n",
       "         [-0.0232],\n",
       "         [ 0.0445],\n",
       "         [ 0.0974],\n",
       "         [ 0.0416],\n",
       "         [-0.0036],\n",
       "         [ 0.0238],\n",
       "         [-0.0598],\n",
       "         [ 0.0394],\n",
       "         [-0.0122],\n",
       "         [ 0.0255],\n",
       "         [-0.0238],\n",
       "         [-0.0922],\n",
       "         [ 0.0113],\n",
       "         [-0.0526],\n",
       "         [ 0.0728],\n",
       "         [-0.0865],\n",
       "         [ 0.0887],\n",
       "         [-0.0339],\n",
       "         [-0.0681],\n",
       "         [-0.0861],\n",
       "         [ 0.0862],\n",
       "         [ 0.0038],\n",
       "         [ 0.0300],\n",
       "         [-0.0175],\n",
       "         [ 0.0274],\n",
       "         [-0.0688],\n",
       "         [-0.0649],\n",
       "         [-0.0443],\n",
       "         [-0.0034],\n",
       "         [ 0.0944],\n",
       "         [ 0.0605],\n",
       "         [ 0.0443],\n",
       "         [ 0.0983],\n",
       "         [ 0.0661],\n",
       "         [ 0.0296],\n",
       "         [-0.0891],\n",
       "         [-0.0239],\n",
       "         [ 0.0816],\n",
       "         [-0.0726],\n",
       "         [-0.0116],\n",
       "         [ 0.0135],\n",
       "         [ 0.0931],\n",
       "         [-0.0759],\n",
       "         [ 0.0919],\n",
       "         [-0.0677],\n",
       "         [-0.0308],\n",
       "         [ 0.0440],\n",
       "         [ 0.0477],\n",
       "         [-0.0830],\n",
       "         [-0.0638],\n",
       "         [ 0.0919],\n",
       "         [-0.0505],\n",
       "         [-0.0744],\n",
       "         [ 0.0117],\n",
       "         [-0.0776],\n",
       "         [-0.0755],\n",
       "         [-0.0639],\n",
       "         [-0.0937],\n",
       "         [-0.0362],\n",
       "         [-0.0850],\n",
       "         [ 0.0797],\n",
       "         [-0.0839],\n",
       "         [ 0.0395],\n",
       "         [-0.0833],\n",
       "         [ 0.0557],\n",
       "         [ 0.0317],\n",
       "         [ 0.0015],\n",
       "         [ 0.0476],\n",
       "         [ 0.0638],\n",
       "         [-0.0152],\n",
       "         [-0.0991],\n",
       "         [ 0.0967],\n",
       "         [ 0.0482],\n",
       "         [-0.0593],\n",
       "         [-0.0298],\n",
       "         [-0.0350],\n",
       "         [-0.0350],\n",
       "         [ 0.0689],\n",
       "         [-0.0075],\n",
       "         [-0.0342],\n",
       "         [-0.0419],\n",
       "         [ 0.0745],\n",
       "         [ 0.0227],\n",
       "         [ 0.0780],\n",
       "         [ 0.0317],\n",
       "         [ 0.0680],\n",
       "         [-0.0205],\n",
       "         [ 0.0662],\n",
       "         [ 0.0555],\n",
       "         [ 0.0178],\n",
       "         [ 0.0088],\n",
       "         [-0.0665],\n",
       "         [ 0.0314],\n",
       "         [ 0.0165],\n",
       "         [ 0.0447],\n",
       "         [-0.0031],\n",
       "         [ 0.0873],\n",
       "         [ 0.0672],\n",
       "         [ 0.0714],\n",
       "         [ 0.0196],\n",
       "         [ 0.0789],\n",
       "         [-0.0948],\n",
       "         [ 0.0841],\n",
       "         [-0.0124],\n",
       "         [-0.0218],\n",
       "         [ 0.0596]], requires_grad=True),\n",
       " 'movie_bias': tensor([[ 0.0112,  0.0215, -0.0247,  ..., -0.0204,  0.0083, -0.0075]],\n",
       "        requires_grad=True)}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-99d4765682f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0225,  0.0175,  0.0226,  ..., -0.0237,  0.0049, -0.0219],\n",
      "        [-0.0165,  0.0203,  0.0128,  ...,  0.0158, -0.0180, -0.0186],\n",
      "        [-0.0168, -0.0163,  0.0157,  ..., -0.0185, -0.0104,  0.0068],\n",
      "        ...,\n",
      "        [ 0.0006,  0.0064,  0.0172,  ..., -0.0197, -0.0032,  0.0203],\n",
      "        [-0.0142,  0.0111,  0.0039,  ...,  0.0034, -0.0094, -0.0040],\n",
      "        [-0.0044,  0.0161,  0.0091,  ..., -0.0177,  0.0070,  0.0016]],\n",
      "       requires_grad=True)\n",
      "tensor([[-0.0525, -0.0213,  0.0281,  ...,  0.0745,  0.0821, -0.0697],\n",
      "        [ 0.0364, -0.0202, -0.0738,  ...,  0.0807,  0.0037,  0.0535],\n",
      "        [-0.0724,  0.0746,  0.0950,  ...,  0.0625,  0.0154, -0.0937],\n",
      "        ...,\n",
      "        [-0.0167, -0.0421,  0.0843,  ...,  0.0505,  0.0604, -0.0341],\n",
      "        [-0.0606,  0.0342, -0.0757,  ...,  0.0066,  0.0632, -0.0598],\n",
      "        [-0.0697, -0.0418, -0.0395,  ...,  0.0637,  0.0464,  0.0361]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(t.movie_embed)\n",
    "print(t.user_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class betternet(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, num_features):\n",
    "        super().__init__()\n",
    "        self.user_embed = nn.Embedding(num_users, num_features)\n",
    "        self.movie_embed = nn.Embedding(num_movies, num_features)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.movie_bias = nn.Embedding(num_movies, 1)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.user_embed.weight.data)\n",
    "        nn.init.xavier_uniform_(self.movie_embed.weight.data)\n",
    "        nn.init.xavier_uniform_(self.user_bias.weight.data)\n",
    "        nn.init.xavier_uniform_(self.movie_bias.weight.data)\n",
    "        \n",
    "    def forward(self, categoricals):\n",
    "        users, movies = categoricals[:,0], categoricals[:,1]\n",
    "        u, m, ub, mb = self.user_embed(users), self.movie_embed(movies), self.user_bias(users), self.movie_bias(movies)\n",
    "        out = torch.mm(u, m.transpose(0,1)) + ub + mb.transpose(0,1)\n",
    "        out = torch.sigmoid(out)\n",
    "        return(out * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betternet(\n",
      "  (user_embed): Embedding(610, 50)\n",
      "  (movie_embed): Embedding(9742, 50)\n",
      "  (user_bias): Embedding(610, 1)\n",
      "  (movie_bias): Embedding(9742, 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = betternet(num_users, num_movies, 50)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.tensor(ratings[:,:2].astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
